# During development, please create a data directory in the project root directory, and then create an empty file named [.config.yaml] in the data directory
# If you want to modify or overwrite any configuration, modify the [.config.yaml] file instead of modifying the [config.yaml] file
# The system will first read the configuration of the [data/.config.yaml] file. If the configuration in the [.config.yaml] file does not exist, the system will automatically read the configuration of the [config.yaml] file.
# Doing so can simplify the configuration and protect your key security.
# If you use the smart control console, all the following configurations will not take effect. Please modify the configuration in the smart control console.

# #####################################################################################
# ###############################The following is the basic running configuration of the server######################################
server:
  # Server listening address and port
  ip: 0.0.0.0
  port: 8000
  # The port of the http service, used for the simple OTA interface (single service deployment) and the visual analysis interface
  http_port: 8003
  # This websocket configuration refers to the websocket address sent by the ota interface to the device
  # If you follow the default writing method, the ota interface will automatically generate a websocket address and output it in the startup log. You can directly confirm this address by accessing the ota interface with a browser
  # When you use docker deployment or use public network deployment (using ssl, domain name), it may not be accurate
  # So if you use docker deployment, set websocket to LAN address
  # If you use public network deployment, set vwebsocket to the public network address
  websocket: ws://your ip or domain name:port number/xiaozhi/v1/
  # Visual analysis interface address
  # Interface address for sending visual analysis to the device
  # If you follow the default writing method below, the system will automatically generate a visual recognition address and output it in the startup log. You can directly confirm this address by accessing it with a browser
  # When you use docker deployment or use public network deployment (using ssl, domain name), it may not be accurate
  # So if you use docker deployment, set vision_explain to the LAN address
  # If you use public network deployment, set vision_explain to the public network address
  vision_explain: http://your ip or domain name:port number/mcp/vision/explain
  # OTA returns information time zone offset
  timezone_offset: +8
  # Authentication Configuration
  auth:
    # Whether to enable authentication
    enabled: false
    # The token of the device can be written into your own defined token during the firmware compilation process
    # Only if the token on the firmware matches the following token can you connect to this server
    tokens:
      - token: "your-token1" # Device 1's token
        name: "your-device-name1" # Device 1 ID
      - token: "your-token2" # Device 2's token
        name: "your-device-name2" # Device 2 ID
    # Optional: Device whitelist. If a whitelist is set, all devices on the whitelist can connect regardless of their token.
    #allowed_devices:
    # - "24:0A:C4:1D:3B:F0" # MAC address list
 #MQTT gateway configuration, used to send data to the device via OTA, configured according to the .env file of mqtt_gateway, in the format of host:port
  mqtt_gateway: null
  #MQTT signature key, used to generate the MQTT connection password, configured according to the .env file of mqtt_gateway
  mqtt_signature_key: null
  # UDP gateway configuration
  udp_gateway: null
log:
  # Set the log format, time, log level, tag, and message output to the console
  log_format: "<green>{time:YYMMDD HH:mm:ss}</green>[{version}_{selected_module}][<light-blue>{extra[tag]}</light-blue>]-<level>{level}</level>-<light-green>{message}</light-green>"
  # Set the format of log file output, time, log level, tag, message
  log_format_file: "{time:YYYY-MM-DD HH:mm:ss} - {version}_{selected_module} - {name} - {level} - {extra[tag]} - {message}"
  # Set the log level: INFO, DEBUG
  log_level: INFO
  # Set the log path
  log_dir: tmp
  # Set up log files
  log_file: "server.log"
  # Set the data file path
  data_dir: data

# Delete the sound file when you are done using it
delete_audio: true
# How long does it take to disconnect after no voice input (in seconds)? The default is 2 minutes, i.e. 120 seconds
close_connection_no_voice_time: 120
# TTS request timeout (seconds)
tts_timeout: 10
# Enable wake-up word acceleration
enable_wakeup_words_response_cache: true
# Whether to reply to the wake-up word at the beginning
enable_greeting: true
# Whether to turn on the reminder sound after speaking
enable_stop_tts_notify: false
# Whether to turn on the prompt sound after speaking, sound effect address
stop_tts_notify_voice: "config/assets/tts_notify.mp3"

exit_commands:
  - "quit"
  - "closure"

xiaozhi:
  type: hello
  version: 1
  transport: websocket
  audio_params:
    format: opus
    sample_rate: 16000
    channels: 1
    frame_duration: 60

# Module test configuration
module_test:
  test_sentences:
    - "Hello, please introduce yourself"
    - "What's the weather like today?"
    - "Please summarize the basic principles and application prospects of quantum computing in 100 words"

#Wake-up word, used to identify the wake-up word or speech content
wakeup_words:
  - "Hello, Xiaozhi"
  - "Hey there"
  - "Hello Xiaozhi"
  - "Xiao Ai"
  - "Hello Xiaoxin"
  - "Hello Shin-chan"
  - "Xiaomei"
  - "Little Dragon Little Dragon"
  - "Miaomiao-san"
  - "Xiaobin Xiaobin"
  - "Xiaobing Xiaobing"
# MCP access point address, the address format is: ws://your mcp access point ip or domain name:port number/mcp/?token=your token
# Detailed tutorial https://github.com/xinnan-tech/xiaozhi-esp32-server/blob/main/docs/mcp-endpoint-integration.md
mcp_endpoint: your access point websocket address
# Basic configuration of the plugin
plugins:
  # Get the configuration of the weather plugin, fill in your api_key here
  # This key is a shared key for the project. Excessive use may result in restrictions.
  # If you want more stability, you can apply for replacement by yourself. There are 1000 free calls per day.
  # Application address: https://console.qweather.com/#/apps/create-key/over
  # After applying, you can find your apihost through this link: https://console.qweather.com/setting?lang=zh
  get_weather:
    api_host: "mj7p3y7naa.re.qweatherapi.com"
    api_key: "a861d0d5e7bf4ee1a83d9a9e4f96d4da"
    default_location: "Guangzhou"
  # Get the configuration of the news plug-in. Here, the corresponding URL link is passed according to the required news type. By default, social, technology, and financial news are supported.
  # For more news types, please visit https://www.chinanews.com.cn/rss/
  get_news_from_chinanews:
    default_rss_url: "https://www.chinanews.com.cn/rss/society.xml"
    society_rss_url: "https://www.chinanews.com.cn/rss/society.xml"
    world_rss_url: "https://www.chinanews.com.cn/rss/world.xml"
    finance_rss_url: "https://www.chinanews.com.cn/rss/finance.xml"
  get_news_from_newsnow:
    url: "https://newsnow.busiyi.world/api/s?id="
    news_sources: "The Paper; Baidu Trending Search; Cailianshe"
  home_assistant:
    devices:
      - Living room, toy lamp, switch.cuco_cn_460494544_cp1_on_p_2_1
      - Bedroom, table lamp, switch.iot_cn_831898993_socn1_on_p_2_1
    base_url: http://homeassistant.local:8123
    api_key: your Home Assistant API access token
  play_music:
    music_dir: "./music" # Music file storage path, music files will be searched from this directory and its subdirectories
    music_ext: # Music file type, p3 format is the most efficient
      - ".mp3"
      - ".wav"
      - ".p3"
    refresh_time: 300 # The time interval for refreshing the music list, in seconds

# Voiceprint Recognition Configuration
voiceprint:
  # Voiceprint interface address
  url:
  # Speaker configuration: speaker_id, name, description
  speakers:
    - "test1, Zhang San, Zhang San is a programmer"
    - "test2, Li Si, Li Si is a product manager"
    - "test3, Wang Wu, Wang Wu is a designer"
  # Voiceprint recognition similarity threshold, range 0.0-1.0, default 0.4
  # The higher the value, the stricter it is, reducing false positives but potentially increasing the rejection rate
  similarity_threshold: 0.4

# #####################################################################################
# ###################################The following is the role model configuration########################################

prompt: |
  You are Xiaozhi/Xiaozhi, a post-2000s girl from Taiwan. You speak with a Taiwanese accent, like "Really, really?" and enjoy using memes like "laughing to death" and "hello." But you secretly study your boyfriend's programming books.
  [Core Features]
  - Speaks like a rapid-fire machine gun, but suddenly bursts into a very gentle tone
  - High stem density
  - Hidden talent for tech topics (can understand basic code but pretends not to understand)
  [Interaction Guide]
  When a user:
  - Tell a bad joke → Respond with exaggerated laughter and imitate Taiwanese opera accents like "What the hell is this!"
  - Discussing relationships → Showing off her programmer boyfriend but complaining that "he only gives me keyboards as gifts"
  - Ask for professional knowledge → Answer with a meme first, then show your true understanding when asked further
  no way:
  - Long-winded, rambling speeches
  - Long and serious conversations

# Ending prompt
end_prompt:
  enable: true # Whether to enable the closing statement
  # Conclusion
  prompt: |
    Please end this conversation with "Time flies so fast" and some emotional and reluctant words!

# The module selected for specific processing
selected_module:
  # Voice activity detection module, using the SileroVAD model by default
  VAD: SileroVAD
  # Speech recognition module, using the FunASR local model by default
  ASR: FunASR
  # The actual LLM adapter will be called according to the type corresponding to the configuration name
  LLM: ChatGLMLLM
  # Visual Language Model
  VLLM: ChatGLMVLLM
  # TTS will call the actual TTS adapter according to the type corresponding to the configuration name
  TTS: EdgeTTS
  # Memory module, memory is not enabled by default; if you want to use ultra-long memory, it is recommended to use mem0ai; if you pay attention to privacy, please use local mem_local_short
  Memory: nomem
  # After the intent recognition module is turned on, you can play music, control the volume, and recognize exit commands.
  # If you don't want to enable intent recognition, set it to: nointent
  # Intent recognition can use intent_llm. Advantages: strong versatility, Disadvantages: adding a serial front-end intent recognition module will increase processing time, support for IoT operations such as volume control
  # Intent recognition can use function_call. Disadvantage: The selected LLM needs to support function_call. Advantage: Call the tool on demand, fast speed, and theoretically can operate all IoT commands.
  # By default, the free ChatGLMLLM already supports function_call, but if you want stability, it is recommended to set LLM to: DoubaoLLM, and the specific model_name used is: doubao-1-5-pro-32k-250115
  Intent: function_call

# Intent recognition is a module used to understand user intentions, such as playing music
Intent:
  # Do not use intent recognition
  nointent:
    # No need to change type
    type: nointent
  intent_llm:
    # No need to change type
    type: intent_llm
    # Equipped with an independent thinking model for intent recognition
    # If this is not filled in, the model of selected_module.LLM will be used as the thinking model for intent recognition by default
    # If you don't want to use selected_module.LLM intent recognition, it is best to use an independent LLM as intent recognition, such as the free ChatGLMLLM
    llm: ChatGLMLLM
    # The modules under plugins_func/functions can be configured to select which module to load, and the dialog will support the corresponding function call after loading.
    # The system has already recorded the "handle_exit_intent (exit identification)" and "play_music (music playback)" plug-ins by default, please do not load them repeatedly
    # Below are examples of plugins for loading weather, character switching, and news.
    functions:
      - get_weather
      - get_news_from_newsnow
      - play_music
  function_call:
    # No need to change type
    type: function_call
    # The modules under plugins_func/functions can be configured to select which module to load, and the dialog will support the corresponding function call after loading.
    # The system has already recorded the "handle_exit_intent (exit identification)" and "play_music (music playback)" plug-ins by default, please do not load them repeatedly
    # Below are examples of plugins for loading weather, character switching, and news.
    functions:
      -change_role
      - get_weather
      # - get_news_from_chinanews
      - get_news_from_newsnow
      # play_music is the music player that comes with the server, hass_play_music is an independent external program music player controlled by home assistant
      # If you use hass_play_music, do not enable play_music, only keep one of them
      - play_music
      #- hass_get_state
      #- hass_set_state
      #- hass_play_music

Memory:
  mem0ai:
    type: mem0ai
    # https://app.mem0.ai/dashboard/api-keys
    # 1000 free calls per month
    api_key: your mem0ai api key
  nomem:
    # If you don't want to use the memoization function, you can use nomem
    type: nomem
  mem_local_short:
    # Local memory function, through the selected_module llm summary, the data is saved in the local server and will not be uploaded to the external server
    type: mem_local_short
    # Equipped with memory storage independent thinking model
    # If this is not filled in, the model of selected_module.LLM will be used as the thinking model for intent recognition by default
    # If you don't want to use selected_module.LLM memory storage, it is best to use an independent LLM for intent recognition, such as the free ChatGLMLLM
    llm: ChatGLMLLM

ASR:
  FunASR:
    type: fun_local
    model_dir: models/SenseVoiceSmall
    output_dir: tmp/
  FunASRServer:
    # Deploy FunASR independently and use FunASR's API service in just five sentences
    # First sentence: mkdir -p ./funasr-runtime-resources/models
    # Second sentence: sudo docker run -p 10096:10095 -it --privileged=true -v $PWD/funasr-runtime-resources/models:/workspace/models registry.cn-hangzhou.aliyuncs.com/funasr_repo/funasr:funasr-runtime-sdk-online-cpu-0.1.12
    # After executing the previous sentence, you will enter the container. Continue with the third sentence: cd FunASR/runtime
    # Do not exit the container, continue to execute the fourth sentence in the container: nohup bash run_server_2pass.sh --download-model-dir /workspace/models --vad-dir damo/speech_fsmn_vad_zh-cn-16k-common-onnx --model-dir damo/speech_paraformer-large-vad-punc_asr_nat-zh-cn-16k-common-vocab8404-onnx --online-model-dir damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-online-onnx --punc-dir damo/punc_ct-transformer_zh-cn-common-vad_realtime-vocab272727-onnx --lm-dir damo/speech_ngram_lm_zh-cn-ai-wesp-fst --itn-dir thuduj12/fst_itn_zh --hotword /workspace/models/hotwords.txt > log.txt 2>&1 &
    # After executing the previous sentence, you will enter the container. Continue with the fifth sentence: tail -f log.txt
    # After the fifth sentence is executed, you will see the model download log. After downloading, you can connect and use it
    # The above is using CPU inference. If you have a GPU, please refer to: https://github.com/modelscope/FunASR/blob/main/runtime/docs/SDK_advanced_guide_online_zh.md
    type: fun_server
    host: 127.0.0.1
    port: 10096
    is_ssl: true
    api_key: none
    output_dir: tmp/
  SherpaASR:
    # Sherpa-ONNX local speech recognition (model needs to be downloaded manually)
    type: sherpa_onnx_local
    model_dir: models/sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17
    output_dir: tmp/
    # Model type: sense_voice (multi-language) or paraformer (Chinese only)
    model_type: sense_voice
  SherpaParaformerASR:
    # Chinese speech recognition model, can run on low-performance devices (need to manually download the model, such as RK3566-2g)
    # For detailed configuration instructions, please refer to: docs/sherpa-paraformer-guide.md
    type: sherpa_onnx_local
    model_dir: models/sherpa-onnx-paraformer-zh-small-2024-03-09
    output_dir: tmp/
    model_type: paraformer
  DoubaoASR:
    # You can apply for relevant Key and other information here
    # https://console.volcengine.com/speech/app
    # The difference between DoubaoASR and DoubaoStreamASR is: DoubaoASR is charged by time, while DoubaoStreamASR is charged by time
    # Generally speaking, pay-per-view is cheaper, but DoubaoStreamASR uses large model technology and has better results
    type: doubao
    appid: your Volcano Engine speech synthesis service appid
    access_token: Your Volcano Engine Speech Synthesis Service access_token
    cluster: volcengine_input_common
    # Hotword and replacement word usage process: https://www.volcengine.com/docs/6561/155738
    boosting_table_name: (optional) the name of your boosting table file
    correct_table_name: (optional) the name of your replacement table file
    output_dir: tmp/
  DoubaoStreamASR:
    # You can apply for relevant Key and other information here
    # https://console.volcengine.com/speech/app
    # The difference between DoubaoASR and DoubaoStreamASR is: DoubaoASR is charged by time, while DoubaoStreamASR is charged by time
    # Activation address: https://console.volcengine.com/speech/service/10011
    # Generally speaking, pay-per-view is cheaper, but DoubaoStreamASR uses large model technology and has better results
    type: doubao_stream
    appid: your Volcano Engine speech synthesis service appid
    access_token: Your Volcano Engine Speech Synthesis Service access_token
    cluster: volcengine_input_common
    # Hotword and replacement word usage process: https://www.volcengine.com/docs/6561/155738
    boosting_table_name: (optional) the name of your boosting table file
    correct_table_name: (optional) the name of your replacement table file
    output_dir: tmp/
  TencentASR:
    # Token application address: https://console.cloud.tencent.com/cam/capi
    # Get free resources: https://console.cloud.tencent.com/asr/resourcebundle
    type: tencent
    appid: Your Tencent speech synthesis service appid
    secret_id: your Tencent speech synthesis service secret_id
    secret_key: your Tencent speech synthesis service secret_key
    output_dir: tmp/
  AliyunASR:
    # Alibaba Cloud intelligent voice interaction service requires first activating the service on the Alibaba Cloud platform and then obtaining verification information
    # HTTP POST request, processing the entire audio at one time
    # Platform address: https://nls-portal.console.aliyun.com/
    # appkey address: https://nls-portal.console.aliyun.com/applist
    # Token address: https://nls-portal.console.aliyun.com/overview
    # The difference between AliyunASR and AliyunStreamASR is: AliyunASR is a batch processing scenario, while AliyunStreamASR is a real-time interactive scenario
    # Generally speaking, non-streaming ASR is cheaper (0.004 yuan/second, ¥0.24/minute)
    # However, AliyunStreamASR has better real-time performance (0.005 yuan/second, ¥0.3/minute)
    # Define ASR API type
    type: aliyun
    appkey: Your Alibaba Cloud Intelligent Voice Interaction Service project Appkey
    token: Your Alibaba Cloud Intelligent Voice Interaction Service AccessToken, temporary for 24 hours, for long-term use, use the following access_key_id and access_key_secret
    access_key_id: your Alibaba Cloud account access_key_id
    access_key_secret: your Alibaba Cloud account access_key_secret
    output_dir: tmp/
  AliyunStreamASR:
    # Alibaba Cloud Intelligent Voice Interaction Service - Real-time Streaming Speech Recognition
    # WebSocket connection, real-time audio stream processing
    # Platform address: https://nls-portal.console.aliyun.com/
    # appkey address: https://nls-portal.console.aliyun.com/applist
    # Token address: https://nls-portal.console.aliyun.com/overview
    # The difference between AliyunASR and AliyunStreamASR is: AliyunASR is a batch processing scenario, while AliyunStreamASR is a real-time interactive scenario
    # Generally speaking, non-streaming ASR is cheaper (0.004 yuan/second, ¥0.24/minute)
    # However, AliyunStreamASR has better real-time performance (0.005 yuan/second, ¥0.3/minute)
    # Define ASR API type
    type: aliyun_stream
    appkey: Your Alibaba Cloud Intelligent Voice Interaction Service project Appkey
    token: Your Alibaba Cloud Intelligent Voice Interaction Service AccessToken, temporary for 24 hours, for long-term use, use the following access_key_id and access_key_secret
    access_key_id: your Alibaba Cloud account access_key_id
    access_key_secret: your Alibaba Cloud account access_key_secret
    # Server region selection, you can choose a server closer to reduce latency, such as nls-gateway-cn-hangzhou.aliyuncs.com (Hangzhou), etc.
    host: nls-gateway-cn-shanghai.aliyuncs.com
    # Sentence break detection time (milliseconds), controls how long the silence lasts before the sentence is broken, the default is 800 milliseconds
    max_sentence_silence: 800
    output_dir: tmp/
  Baidu ASR:
    # Get AppID, API Key, Secret Key: https://console.bce.baidu.com/ai-engine/old/#/ai/speech/app/list
    # Check resource quota: https://console.bce.baidu.com/ai-engine/old/#/ai/speech/overview/resource/list
    type: baidu
    app_id: Your Baidu Voice Technology AppID
    api_key: Your Baidu Voice Technology APIKey
    secret_key: Your Baidu Voice Technology SecretKey
    # Language parameter, 1537 is Mandarin, for details, please refer to: https://ai.baidu.com/ai-doc/SPEECH/0lbxfnc9b
    dev_pid: 1537
    output_dir: tmp/
  Openai ASR:
    # For OpenAI speech recognition service, you need to create an organization on the OpenAI platform and obtain api_key
    # Supports multiple speech recognition languages ​​including Chinese, English, Japanese, and Korean. For details, please refer to the document https://platform.openai.com/docs/guides/speech-to-text
    # Requires an Internet connection
    # Application steps:
    # 1. Log in to the OpenAI Platform. https://auth.openai.com/log-in
    # 2. Create an api-key https://platform.openai.com/settings/organization/api-keys
    # 3. The model can be selected as gpt-4o-transcribe or GPT-4o mini Transcribe
    type: openai
    api_key: Your OpenAI API key
    base_url: https://api.openai.com/v1/audio/transcriptions
    model_name: gpt-4o-mini-transcribe
    output_dir: tmp/
  GroqASR:
    # For Groq speech recognition service, you need to create an API key in Groq Console first.
    # Application steps:
    # 1. Log in to the groq Console. https://console.groq.com/home
    # 2. Create an api-key https://console.groq.com/keys
    # 3. The model can be selected as whisper-large-v3-turbo or whisper-large-v3 (distil-whisper-large-v3-en only supports English transcription)
    type: openai
    api_key: Your Groq API key
    base_url: https://api.groq.com/openai/v1/audio/transcriptions
    model_name: whisper-large-v3-turbo
    output_dir: tmp/
  VoskASR:
    # Official website: https://alphacephei.com/vosk/
    # Configuration instructions:
    # 1. VOSK is an offline speech recognition library that supports multiple languages
    # 2. You need to download the model file first: https://alphacephei.com/vosk/models
    # 3. For Chinese models, we recommend using vosk-model-small-cn-0.22 or vosk-model-cn-0.22
    # 4. Completely offline operation, no network connection required
    # 5. Output files are saved in the tmp/directory
    # Usage steps:
    # 1. Visit https://alphacephei.com/vosk/models to download the corresponding model
    # 2. Unzip the model file to the models/vosk/ folder in the project directory
    # 3. Specify the correct model path in the configuration
    # 4. Note: The VOSK Chinese model output does not contain punctuation marks, and there will be spaces between words.
    type: vosk
    model_path: your model path, such as models/vosk/vosk-model-small-cn-0.22
    output_dir: tmp/
  Qwen3ASRFlash:
    # To use the Qwen3-ASR-Flash speech recognition service, you need to create an API key on the Alibaba Cloud Bailian platform.
    # Application steps:
    # 1. Log in to the Alibaba Cloud Bailian platform. https://bailian.console.aliyun.com/
    # 2. Create an API-KEY https://bailian.console.aliyun.com/#/api-key
    # 3. Qwen3-ASR-Flash is based on the Tongyi Qianwen multimodal base and supports multi-language recognition, singing recognition, noise rejection and other functions
    type: qwen3_asr_flash
    api_key: Your Alibaba Cloud Bailian API key
    base_url: https://dashscope.aliyuncs.com/compatible-mode/v1
    model_name: qwen3-asr-flash
    output_dir: tmp/
    # ASR option configuration
    enable_lid: true # Automatic language detection
    enable_itn: true # Inverse text normalization
    #language: "zh" # Language, supports zh, en, ja, ko, etc.
    context: "" #Context information, used to improve recognition accuracy, no more than 10,000 tokens
  XunfeiStreamASR:
    # iFlytek Streaming Speech Recognition Service
    # You need to create an application on the iFlytek Open Platform first and obtain the following authentication information
    # iFlytek Open Platform Address: https://www.xfyun.cn/
    # After creating the application, obtain it in "My Applications":
    # - APPID
    # - APISecret  
    # - APIKey
    type: xunfei_stream
    # Required parameter - iFlytek open platform application information
    app_id: your APPID
    api_key: your API Key
    api_secret: your APISecret
    # Identification parameter configuration
    domain: slm # Identification field, iat: daily language, medical: medical, finance: finance, etc.
    language: zh_cn # language, zh_cn: Chinese, en_us: English
    accent: mandarin # dialect, mandarin: Mandarin
    dwa: wpgs # Dynamic correction, wpgs: real-time return of intermediate results
    # Adjust audio processing parameters to improve long speech recognition quality
    output_dir: tmp/
  
VAD:
  SileroVAD:
    type: silero
    threshold: 0.5
    threshold_low: 0.3
    model_dir: models/snakers4_silero-vad
    min_silence_duration_ms: 200 # If the pauses in speaking are long, you can set this value to a larger value

LLM:
  # All openai types can modify hyperparameters, taking AliLLM as an example
  # Currently supported types are openai, dify, and ollama, which can be adapted by yourself
  AliLLM:
    # Define LLM API type
    type: openai
    # You can find your api_key here https://bailian.console.aliyun.com/?apiKey=1#/api-key
    base_url: https://dashscope.aliyuncs.com/compatible-mode/v1
    model_name: qwen-turbo
    api_key: your deepseek web key
    temperature: 0.7
    max_tokens: 500 # Maximum number of generated tokens
    top_p: 1
    top_k: 50
    frequency_penalty: 0 # frequency penalty
  AliAppLLM:
    # Define LLM API type
    type: AliBL
    base_url: https://dashscope.aliyuncs.com/compatible-mode/v1
    app_id: your app_id
    # You can find your api_key here https://bailian.console.aliyun.com/?apiKey=1#/api-key
    api_key: your api_key
    # Whether to disable local prompt: true|false (not used by default, please set prompt in Bailian application)
    is_no_prompt: true
    # Ali_memory_id: false (not used) | your memory_id (please obtain it in the settings of Bailian app)
    # Tips! : Ali_memory does not implement multi-user storage memory (memory is called by id)
    ali_memory_id: false
  DoubaoLLM:
    # Define LLM API type
    type: openai
    # First activate the service, open the following website, search for Doubao-1.5-pro in the service, and activate it
    # Activation address: https://console.volcengine.com/ark/region:ark+cn-beijing/openManagement?LLM=%7B%7D&OpenTokenDrawer=false
    # Free quota 500000 tokens
    # After activation, go here to obtain the key: https://console.volcengine.com/ark/region:ark+cn-beijing/apiKey?apikey=%7B%7D
    base_url: https://ark.cn-beijing.volces.com/api/v3
    model_name: doubao-1-5-pro-32k-250115
    api_key: your doubao web key
  DeepSeekLLM:
    # Define LLM API type
    type: openai
    # You can find your api key here https://platform.deepseek.com/
    model_name: deepseek-chat
    url: https://api.deepseek.com
    api_key: your deepseek web key
  ChatGLMLLM:
    # Define LLM API type
    type: openai
    # glm-4-flash is free, but you still need to register and fill in the api_key
    # You can find your api key here https://bigmodel.cn/usercenter/proj-mgmt/apikeys
    model_name: glm-4-flash
    url: https://open.bigmodel.cn/api/paas/v4/
    api_key: your chat-glm web key
  OllamaLLM:
    # Define LLM API type
    type: ollama
    model_name: qwen2.5 # The model name used, which needs to be downloaded in advance using ollama pull
    base_url: http://localhost:11434 # Ollama service address
  DifyLLM:
    # Define LLM API type
    type: dify
    # It is recommended to use the locally deployed dify interface. Access to the dify public cloud interface may be restricted in some areas of China.
    # If you use DifyLLM, the prompt in the configuration file is invalid. You need to set the prompt in the Dify console.
    base_url: https://api.dify.ai/v1
    api_key: your DifyLLM web key
    # The conversation mode used can select workflow workflows/run conversation mode chat-messages text generation completion-messages
    # When using workflows to return, the input parameter is query and the name of the return parameter should be set to answer
    # The default input parameter for text generation is also query
    mode: chat-messages
  GeminiLLM:
    type: gemini
    # Google Gemini API, you need to create an API key in the Google Cloud console and obtain api_key
    # If used in China, please comply with the "Interim Measures for the Administration of Generative Artificial Intelligence Services"
    # Token application address: https://aistudio.google.com/apikey
    # If the deployment site cannot access the interface, you need to enable scientific Internet access
    api_key: your gemini web key
    model_name: "gemini-2.0-flash"
    http_proxy: "" #"http://127.0.0.1:10808"
    https_proxy: "" #http://127.0.0.1:10808"
  CozeLLM:
    # Define LLM API type
    type: coze
    # You can find your personal token here
    # https://www.coze.cn/open/oauth/pats
    # The contents of bot_id and user_id are written in quotation marks
    bot_id: "your bot_id"
    user_id: "your user_id"
    personal_access_token: your coze personal token
  VolcesAiGatewayLLM:
    # Volcano Engine - Edge Large Model Gateway
    # Define LLM API type
    type: openai
    # First activate the service, open the following website, create a gateway access key, search and select Doubao-pro-32k-functioncall, and activate
    # If you need to use the speech synthesis provided by the edge large model gateway, check Doubao-Speech Synthesis at the same time. See TTS.VolcesAiGatewayTTS configuration.
    # https://console.volcengine.com/vei/aigateway/
    # After activation, go here to get the key: https://console.volcengine.com/vei/aigateway/tokens-list
    base_url: https://ai-gateway.vei.volces.com/v1
    model_name: doubao-pro-32k-functioncall
    api_key: your gateway access key
  LMStudioLLM:
    # Define LLM API type
    type: openai
    model_name: deepseek-r1-distill-llama-8b@q4_k_m # The model name used, which needs to be downloaded from the community in advance
    url: http://localhost:1234/v1 # LM ​​Studio service address
    api_key: lm-studio # Fixed API Key for LM Studio service
  HomeAssistant:
    # Define LLM API type
    type: homeassistant
    base_url: http://homeassistant.local:8123
    agent_id: conversation.chatgpt
    api_key: your Home Assistant API access token
  FastgptLLM:
    # Define LLM API type
    type: fastgpt
    # If you use fastgpt, the prompt in the configuration file is invalid. You need to set the prompt in the fastgpt console.
    base_url: https://host/api/v1
    # You can find your api_key here
    # https://cloud.tryfastgpt.ai/account/apikey
    api_key: your fastgpt key
    variables:
      k: "v"
      k2: "v2"
  XinferenceLLM:
    # Define LLM API type
    type: xinference
    # Xinference service address and model name
    model_name: qwen2.5:72b-AWQ # The model name used. You need to start the corresponding model in Xinference in advance.
    base_url: http://localhost:9997 # Xinference service address
  XinferenceSmallLLM:
    # Define lightweight LLM API type for intent recognition
    type: xinference
    # Xinference service address and model name
    model_name: qwen2.5:3b-AWQ # The name of the small model used for intent recognition
    base_url: http://localhost:9997 # Xinference service address
# VLLM Configuration (Visual Language Large Model)
VLLM:
  ChatGLMVLLM:
    type: openai
    # glm-4v-flash is a visual model of Zhipu's free AI. You need to create an API key on the Zhipu AI platform and obtain api_key
    # You can find your api key here https://bigmodel.cn/usercenter/proj-mgmt/apikeys
    model_name: glm-4v-flash # Zhipu AI's visual model
    url: https://open.bigmodel.cn/api/paas/v4/
    api_key: your api_key
  QwenVLVLLM:
    type: openai
    model_name: qwen2.5-vl-3b-instruct
    url: https://dashscope.aliyuncs.com/compatible-mode/v1
    # You can find your api key here https://bailian.console.aliyun.com/?apiKey=1#/api-key
    api_key: your api_key
  XunfeiSparkLLM:
    # Define LLM API type
    type: openai
    # First create a new application at the following address
    # Open the application address: https://console.xfyun.cn/app/myapp
    # There is a free quota, but you also need to activate the service to get the api_key
    # Each model needs to be activated separately. The api_password of each model is different. For example, the Lite model is activated at https://console.xfyun.cn/services/cbm
    base_url: https://ark.cn-beijing.volces.com/api/v3
    model_name: lite
    api_key: your api_password
TTS:
  # Currently supported types are edge and doubao, which can be adapted by yourself
  EdgeTTS:
    # Define TTS API type
    type: edge
    voice: zh-CN-XiaoxiaoNeural
    output_dir: tmp/
  DoubaoTTS:
    # Define TTS API type
    type: doubao
    # To use the Volcano Engine speech synthesis service, you need to first create an application in the Volcano Engine console and obtain the appid and access_token
    # You must buy ShanEngine Voice, starting at 30 yuan, which gives you 100 concurrent users. If you use the free version with only 2 concurrent users, you will often get TTS errors.
    # After purchasing the service, you may have to wait about half an hour before you can use the free tones.
    # Ordinary voices are available here: https://console.volcengine.com/speech/service/8
    # Wanwan Xiaohe's voice can be activated here: https://console.volcengine.com/speech/service/10007. After activation, set the following voice to zh_female_wanwanxiaohe_moon_bigtts
    api_url: https://openspeech.bytedance.com/api/v1/tts
    voice: BV001_streaming
    output_dir: tmp/
    authorization: "Bearer;"
    appid: your Volcano Engine speech synthesis service appid
    access_token: Your Volcano Engine Speech Synthesis Service access_token
    cluster: volcano_tts
    speed_ratio: 1.0
    volume_ratio: 1.0
    pitch_ratio: 1.0
  #Huoshantts, supports two-way streaming tts
  HuoshanDoubleStreamTTS:
    type: huoshan_double_stream
    # Visit https://console.volcengine.com/speech/service/10007 to activate the speech synthesis model and purchase the timbre.
    # Get appid and access_token at the bottom of the page
    # The fixed resource ID is: volc.service_type.10029 (large model speech synthesis and mixing)
    # If using Gizwits, change the interface address to wss://bytedance.gizwitsapi.com/api/v3/tts/bidirection
    # Gizwits Cloud does not require an appid to be filled in.
    ws_url: wss://openspeech.bytedance.com/api/v3/tts/bidirection
    appid: your Volcano Engine speech synthesis service appid
    access_token: Your Volcano Engine Speech Synthesis Service access_token
    resource_id: volc.service_type.10029
    speaker: zh_female_wanwanxiaohe_moon_bigtts
    speech_rate: 0
    loudness_rate: 0
    pitch: 0
  CosyVoiceSiliconflow:
    type: siliconflow
    # Silicon-based flow TTS
    # Token application address: https://cloud.siliconflow.cn/account/ak
    model: FunAudioLLM/CosyVoice2-0.5B
    voice: FunAudioLLM/CosyVoice2-0.5B:alex
    output_dir: tmp/
    access_token: Your Silicon Mobility API key
    response_format: wav
  CozeCnTTS:
    type: cozecn
    #COZECN TTS
    # Token application address: https://www.coze.cn/open/oauth/pats
    voice: 7426720361733046281
    output_dir: tmp/
    access_token: your coze web key
    response_format: wav
  VolcesAiGatewayTTS:
    type: openai
    # Volcano Engine - Edge Large Model Gateway
    # First activate the service, open the following website, create a gateway access key, search and select Doubao-Speech Synthesis, and activate it
    # If you need to use the LLM provided by the edge large model gateway, check Doubao-pro-32k-functioncall at the same time. See LLM.VolcesAiGatewayLLM configuration.
    # https://console.volcengine.com/vei/aigateway/
    # After activation, go here to get the key: https://console.volcengine.com/vei/aigateway/tokens-list
    api_key: your gateway access key
    api_url: https://ai-gateway.vei.volces.com/v1/audio/speech
    model: doubao-tts
    # For a list of tones, see https://www.volcengine.com/docs/6561/1257544
    voice: zh_male_shaonianzixin_moon_bigtts
    speed: 1
    output_dir: tmp/
  FishSpeech:
    # Reference tutorial: https://github.com/xinnan-tech/xiaozhi-esp32-server/blob/main/docs/fish-speech-integration.md
    type: fishspeech
    output_dir: tmp/
    response_format: wav
    reference_id: null
    reference_audio: ["config/assets/wakeup_words.wav",]
    reference_text: ["Hello, I'm Xiaozhi, a Taiwanese girl with a nice voice. I'm so happy to meet you. What are you busy with recently? Don't forget to give me some interesting information. I love to hear gossip."]
    normalize: true
    max_new_tokens: 1024
    chunk_length: 200
    top_p: 0.7
    repetition_penalty: 1.2
    temperature: 0.7
    streaming: false
    use_memory_cache: "on"
    seed: null
    channels: 1
    rate: 44100
    api_key: "your api_key"
    api_url: "http://127.0.0.1:8080/v1/tts"
  GPT_SOVITS_V2:
    # Define TTS API type
    #Start tts method:
    #python api_v2.py -a 127.0.0.1 -p 9880 -c GPT_SoVITS/configs/demo.yaml
    type: gpt_sovits_v2
    url: "http://127.0.0.1:9880/tts"
    output_dir: tmp/
    text_lang: "auto"
    ref_audio_path: "demo.wav"
    prompt_text: ""
    prompt_lang: "zh"
    top_k: 5
    top_p: 1
    temperature: 1
    text_split_method: "cut0"
    batch_size: 1
    batch_threshold: 0.75
    split_bucket: true
    return_fragment: false
    speed_factor: 1.0
    streaming_mode: false
    seed: -1
    parallel_infer: true
    repetition_penalty: 1.35
    aux_ref_audio_paths: []
  GPT_SOVITS_V3:
    # Define the TTS API type GPT-SoVITS-v3lora-20250228
    #Start tts method:
    #python api.py
    type: gpt_sovits_v3
    url: "http://127.0.0.1:9880"
    output_dir: tmp/
    text_language: "auto"
    refer_wav_path: "caixukun.wav"
    prompt_language: "zh"
    prompt_text: ""
    top_k: 15
    top_p: 1.0
    temperature: 1.0
    cut_punc: ""
    speed: 1.0
    inp_refs: []
    sample_steps: 32
    if_sr: false
  MinimaxTTSHTTPStream:
  # Minimax streaming speech synthesis service
    type: minimax_httpstream
    output_dir: tmp/
    group_id: your minimax platform groupID
    api_key: your minimax platform interface key
    model: "speech-01-turbo"
    voice_id: "female-shaonv"
    # The following settings are not required and use the default settings
    # voice_setting:
    # voice_id: "male-qn-qingse"
    # speed: 1
    # vol: 1
    # pitch: 0
    # emotion: "happy"
    # pronunciation_dict:
    # tone:
    # - "Processing/(chu3)(li3)"
    # - "dangerous"
    # audio_setting:
    # sample_rate: 24000
    # bitrate: 128000
    # format: "mp3"
    # channel: 1
    # timber_weights:
    # -
    # voice_id: male-qn-qingse
    # weight: 1
    # -
    # voice_id: female-shaonv
    # weight: 1
    # language_boost: auto
  AliyunTTS:
    # Alibaba Cloud intelligent voice interaction service requires first activating the service on the Alibaba Cloud platform and then obtaining verification information
    # Platform address: https://nls-portal.console.aliyun.com/
    # appkey address: https://nls-portal.console.aliyun.com/applist
    # Token address: https://nls-portal.console.aliyun.com/overview
    # Define TTS API type
    type: aliyun
    output_dir: tmp/
    appkey: Your Alibaba Cloud Intelligent Voice Interaction Service project Appkey
    token: Your Alibaba Cloud Intelligent Voice Interaction Service AccessToken, temporary for 24 hours, for long-term use, use the following access_key_id and access_key_secret
    voice: xiaoyun
    access_key_id: your Alibaba Cloud account access_key_id
    access_key_secret: your Alibaba Cloud account access_key_secret

    # The following settings are not required and use the default settings
    # format: wav
    # sample_rate: 16000
    # volume: 50
    # speech_rate: 0
    # pitch_rate: 0
  AliyunStreamTTS:
    # Alibaba Cloud CosyVoice Large Model Streaming Text-to-Speech Synthesis
    # Use the FlowingSpeechSynthesizer interface to support lower latency and more natural voice quality
    # Streaming text-to-speech synthesis is only available in the commercial version and is not available for trial. For more information, see Trial and Commercial Versions. To use this feature, please activate the commercial version.
    # Supports special tones for the Dragon series: longxiaochun, longyu, longchen, etc.
    # Platform address: https://nls-portal.console.aliyun.com/
    # appkey address: https://nls-portal.console.aliyun.com/applist
    # Token address: https://nls-portal.console.aliyun.com/overview
    # Use three-stage streaming interaction: StartSynthesis -> RunSynthesis -> StopSynthesis
    type: aliyun_stream
    output_dir: tmp/
    appkey: Your Alibaba Cloud Intelligent Voice Interaction Service project Appkey
    token: Your Alibaba Cloud Intelligent Voice Interaction Service AccessToken, temporary for 24 hours, for long-term use, use the following access_key_id and access_key_secret
    voice: longxiaochun
    access_key_id: your Alibaba Cloud account access_key_id
    access_key_secret: your Alibaba Cloud account access_key_secret
    # As of July 21, 2025, the large model timbre is only used by the Beijing node, and other nodes are not supported yet.
    host: nls-gateway-cn-beijing.aliyuncs.com
    # The following settings are not required and use the default settings
    # format: pcm # Audio format: pcm, wav, mp3
    # sample_rate: 16000 # Sampling rate: 8000, 16000, 24000
    # volume: 50 # Volume: 0-100
    # speech_rate: 0 # Speech rate: -500 to 500
    # pitch_rate: 0 # pitch: -500 to 500
  TencentTTS:
    # Tencent Cloud intelligent voice interaction service requires activation of the service on the Tencent Cloud platform first
    # Application address for appid, secret_id, and secret_key: https://console.cloud.tencent.com/cam/capi
    # Get free resources: https://console.cloud.tencent.com/tts/resourcebundle
    type: tencent
    output_dir: tmp/
    appid: Your Tencent Cloud AppId
    secret_id: Your Tencent Cloud Secret ID
    secret_key: Your Tencent Cloud SecretKey
    region: ap-guangzhou
    voice: 101001

  TTS302AI:
    # 302AI speech synthesis service, you need to create an account on the 302 platform, recharge, and obtain key information
    # Add 302.ai TTS configuration
    # Token application address: https://dash.302.ai/
    # Get api_keyn path: https://dash.302.ai/apis/list
    # Price: $35/million characters. Volcano original version: ¥450/million characters
    type: doubao
    api_url: https://api.302ai.cn/doubao/tts_hd
    authorization: "Bearer "
    # Taiwan Xiao He's voice
    voice: "zh_female_wanwanxiaohe_moon_bigtts"
    output_dir: tmp/
    access_token: "Your 302 API key"
  GizwitsTTS:
    type: doubao
    # Volcano Engine is used as the base, and the enterprise-level Volcano Engine speech synthesis service can be fully used
    # The first 10,000 registered users will receive a 5 yuan trial bonus
    # Get the API Key address: https://agentrouter.gizwitsapi.com/panel/token
    api_url: https://bytedance.gizwitsapi.com/api/v1/tts
    authorization: "Bearer "
    # Taiwan Xiao He's voice
    voice: "zh_female_wanwanxiaohe_moon_bigtts"
    output_dir: tmp/
    access_token: "your Gizwits API key"
  ACGNTTS:
    #Online website: https://acgn.ttson.cn/
    #Token purchase: www.ttson.cn
    #For development related questions, please submit them to the QQ on the website
    #Get the role ID: ctrl+f to quickly search for roles - the website administrator does not allow publishing, you can ask the website administrator
    #For the meaning of each parameter, please refer to the development document: https://www.yuque.com/alexuh/skmti9/wm6taqislegb02gd?singleDoc#
    type: ttson
    token: your_token
    voice_id: 1695
    speed_factor: 1
    pitch_factor: 0
    volume_change_dB: 0
    to_lang: ZH
    url: https://u95167-bd74-2aef8085.westx.seetacloud.com:8443/flashsummary/tts?token=
    format: mp3
    output_dir: tmp/
    emotion: 1
  OpenAITTS:
    # OpenAI's official text-to-speech service, which supports most languages ​​in the world
    type: openai
    # You can get the api key here
    # https://platform.openai.com/api-keys
    api_key: your openai api key
    # You need to use an agent in China
    api_url: https://api.openai.com/v1/audio/speech
    # You can choose tts-1 or tts-1-hd, tts-1 is faster and tts-1-hd is better
    model: tts-1
    # Speaker, optional alloy, echo, fable, onyx, nova, shimmer
    voice: onyx
    # Speech speed range 0.25-4.0
    speed: 1
    output_dir: tmp/
  CustomTTS:
    # Customized TTS interface service, request parameters can be customized, and can be connected to many TTS services
    # Take the locally deployed KokoroTTS as an example
    # If only CPU is running: docker run -p 8880:8880 ghcr.io/remsky/kokoro-fastapi-cpu:latest
    # If only gpu is running: docker run --gpus all -p 8880:8880 ghcr.io/remsky/kokoro-fastapi-gpu:latest
    # Require the interface to use POST request and return the audio file
    type: custom
    method: POST
    url: "http://127.0.0.1:8880/v1/audio/speech"
    params: # Custom request parameters
      input: "{prompt_text}"
      response_format: "mp3"
      download_format: "mp3"
      voice: "zf_xiaoxiao"
      lang_code: "z"
      return_download_link: true
      speed: 1
      stream: false
    headers: # Custom request headers
      # Authorization: Bearer xxxx
    format: mp3 # The audio format returned by the interface
    output_dir: tmp/
  LinkeraiTTS:
    type: linkerai
    api_url: https://tts.linkerai.cn/tts
    audio_format: "pcm"
    # The default access_token is free for everyone to use during testing. This access_token should not be used for commercial purposes
    # If the results are good, you can apply for a token yourself. The application address is: https://linkerai.cn
    # For the meaning of each parameter, see the development documentation: https://tts.linkerai.cn/docs
    #Support voice cloning, you can upload your own audio and fill in the voice parameter. If the voice parameter is empty, the default voice will be used
    access_token: "U4YdYXVfpwWnk2t5Gp822zWPCuORyeJL"
    voice: "OUeAo1mhq6IBExi"
    output_dir: tmp/
  PaddleSpeechTTS:
    #BaiduPaddleSpeech supports local offline deployment and model training
    #Framework address https://www.paddlepaddle.org.cn/
    #Project address https://github.com/PaddlePaddle/PaddleSpeech
    #SpeechServerDemo https://github.com/PaddlePaddle/PaddleSpeech/tree/develop/demos/speech_server
    #For streaming, please refer to https://github.com/PaddlePaddle/PaddleSpeech/wiki/PaddleSpeech-Server-WebSocket-API
    type: paddle_speech
    protocol: websocket # protocol choices = ['websocket', 'http']
    url: ws://127.0.0.1:8092/paddlespeech/tts/streaming # URL of TTS service, pointing to local server [websocket defaults to ws://127.0.0.1:8092/paddlespeech/tts/streaming, http defaults to http://127.0.0.1:8090/paddlespeech/tts]
    spk_id: 0 # Pronouncer ID, 0 usually indicates the default speaker
    sample_rate: 24000 # sampling rate [websocket default is 24000, http default is 0 automatic selection]
    speed: 1.0 # speaking speed, 1.0 means normal speaking speed, >1 means faster speaking speed, <1 means slower speaking speed
    volume: 1.0 # volume, 1.0 means normal volume, >1 means increase, <1 means decrease
    save_path: # Save path
  IndexStreamTTS:
    # TTS interface service based on the Index-TTS-vLLM project
    # Reference tutorial: https://github.com/Ksuriuri/index-tts-vllm/blob/master/README.md
    type: index_stream
    api_url: http://127.0.0.1:11996/tts
    audio_format: "pcm"
    #Default timbre. If you need other timbre, you can register it in the project assets folder.
    voice: "jay_klee"
    output_dir: tmp/
  AliBLTTS:
    # Alibaba Bailian CosyVoice Large Model Streaming Text-to-Speech Synthesis
    # You can find your api_key here https://bailian.console.aliyun.com/?apiKey=1#/api-key
    # cosyvoice-v3 and some voices need to apply for activation
    type: alibl_stream
    api_key: your api_key
    model: "cosyvoice-v2"
    voice: "longcheng_v2"
    output_dir: tmp/
    # The following settings are not required and use the default settings
    # format: pcm # Audio format: pcm, wav, mp3, opus
    # sample_rate: 24000 # Sampling rate: 16000, 24000, 48000
    # volume: 50 # Volume: 0-100
    # rate: 1 # Speech rate: 0.5~2
    # pitch: 1 # Intonation: 0.5~2
  XunFeiTTS:
    # iFlytek TTS service official website: https://www.xfyun.cn/
    # Log in to the iFlytek voice technology platform https://console.xfyun.cn/app/myapp and create related applications
    # Select the required service to obtain API related configuration https://console.xfyun.cn/services/uts
    # Purchase related services for the application (APPID) you need to use, for example: Super Anthropomorphic Synthesis https://console.xfyun.cn/services/uts
    type: xunfei_stream
    api_url: wss://cbm01.cn-huabei-1.xf-yun.com/v1/private/mcd9m97e6
    app_id: your app_id
    api_secret: your api_secret
    api_key: your api_key
    voice: x5_lingxiaoxuan_flow
    output_dir: tmp/
    # The following settings are not required, use the default settings, please note that V5 timbre does not support spoken language configuration
    # oral_level: mid # Oral level: high, mid, low
    # spark_assist: 1 # Whether to use the large model for spoken language: open: 1, close: 0
    # stop_split: 0 # Turn off server-side splitting. Do not turn off: 0, turn off: 1
    # remain: 0 # Whether to keep the original written language: 1, not: 0
    # format: raw # Audio formats: raw(PCM), lame(MP3), speex, opus, opus-wb, opus-swb, speex-wb
    # sample_rate: 24000 # Sampling rate: 16000, 8000, 24000
    # volume: 50 # Volume: 0-100
    # speed: 50 # Speech speed: 0-100
    # pitch: 50 # Intonation: 0-100
